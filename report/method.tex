\subsection{Data Used}
We used the 2012  Presidential Debate Corpus care of Boydstun et al. The full version is roughly 70 MB and consists of X unique entries.
For LDA we used gensim, a python library developed by Radim Řehůřek \cite{gensim}

\subsection{Inherent Bias}
The corpus that we are using consists of the reactions of college students to the 2012 presidential debates. Because the data reflects a subset of citizens who tend to be younger and more progressive, we feel it is necessary to state as such at the outset. Any collective refereence to respondants will be framed by this bias.

\subsection{Expected Results}
Since LDA can be thought of as a dimension reduction technique, ideally we wanted to feed in a set of documents (as described below) and get some output back that described the principal components of these documents.

\subsection{Pre-Processing}
The corpus exists as a comma separated value (CSV) file, where each atomic entry is separated by a carriage return, and each value within an entry is separated by a comma. For our purposes, this file can reasonably be thought of as a $MxN$ matrix, where $M$ is the number of entries and $N$ is the max number of values that an single entry could have.

A \emph{document} is then a join operation over this matrix, where document $k$ is formed by the joining of all rows $m \on M$ such that index $m,n$ is some collective identifier.

\emph{Example}: \emph{userID} is the first column of the matrix, and we wish to form documents that correspond to each user that participated in the questionnaire. Then a document is $\all m \on M | n = ID ,C_{m,n}$ In layman's terms, this is just every row with a matching userID.

