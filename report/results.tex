\subsection{Initial Results}
The main problem with using single-line reactions from a CSV-like structure with LDA is data sparsity: if each line is treated as a document, then LDA will see hundreds of thousands of 30-something-word documents where no words are ever repeated. Since LDA fundamentally relies on there being some repeated information (read: words) in order to make an inference on the data, it is necessary to aggregate lines from a CSV together to make some internal aggregate that corresponds to the concept of a 'document' in LDA-speak.\\

Our initial approach was to treat all reactions logged over the entirety of the debate by a unique user to be clustered together; this would correspond to a document representing a single, unique user and topics generated by LDA to represent broad demographic clusters of all the users.

However, there is a lot of 'loud' information present that, when aggregated over many lines to represent a single user, tend to drown out potentially more interesting features.

For example: User $A$ reports that they are watching the debates on the internet. This means that every entry that $A$ logs will have an identical entry of \emph{How: internet}. As a corollary, because $A$ initially logged \emph{How: internet}, Other features that act as conditional follow-ups to the question \emph{How: } are tied to that questions equivalent of 'N/A'. Because of how we chose to aggregate multiple rows together, this lends an extremely high weight to a relatively unimportant feature. To see whether this had any major effect on returned topics, we ran LDA on the unmodified data, and again with unimportant/loud features removed:\\

%insert pic here?
\begin{figure}[H]
\centering

\includegraphics[scale=.3]{img/topic_full.png}
\caption{Two example topics WITHOUT truncation}

%insert pic2 here?

\includegraphics[scale=.3]{img/topic_trunc.png}
\caption{Two example topics WITH truncation}

\end{figure}


Features that were removed were:
\begin{description}
\item['How:'] How users watched debates
\item['PoliAware:'] How Politically Aware Users consider themselves
\item['FavSrc:'] Favorite Source for watching political campaigns
\item['TVChnl:'] TV channel you watch debates on
\item['Pref(Econ)'] What candidate you prefer for Economics
\item['Pref(FP)'] What candidate you prefer for Foreign Policy
\item['Pref(Cand)'] What Candidate you prefer overall
\item['Ready?'] Filler question
\end{description}
 This shrunk the feature vector from $38$ unique features to $29$.


From the topics, we summarize the interesting differences as follows:
\begin{itemize}
\item Close to half of our topics indicated that users were, in fact, ready to begin.
\item Topics that indicated a likelihood to vote for Obama were still critical of Obama.
\item Of topics that included religion, the only answers to be make a difference were Christian, None Specified and Atheist.

\end{itemize}



\subsection{Was LDA the Right Choice?}
Possibly. Because of the document paradigm that underlies an LDA model, CSV formatted responses have to be clustered in some way to represent this as we demonstrated in previous sections. As we discussed previously, one way that we preserved feature information was to move the column header into each feature so that information about WHAT the feature represented was preserved after clustering, and distinguish features with the same original format (eg. values ranging from $1 - 100$ for importance of political topics) so that the bag of words approach LDA assumes is acceptable doesn't produce incomprehensible output.

However, there were some fundamental issues with this representation. In Initial Results we discussed how certain feature columns would overwhelm the LDA model, because the 'word' that the feature represented would be constant or near-constant across all instances in a document. To a lesser extent, any of the features that represent a dial that is left at some untouched, constant value or a feature that has a fundamentally smaller range of values that it can take will be representationally stronger in an LDA model.

By design, word frequency is what drives LDA model. Essentially the tradeoff was one of data transformation (ie. matrix-style response information with high vertical repetition) for linguistic flexibility (ie. multiple topics to explain how a document was formed instead of a deterministic dimension reduction like PCA.)\\

This is best summarized by Radim Řehůřek's (Gensim's author) warning: ``Garbage in, Garbage out.'' If you aren't very careful about what is fed into LDA, you can't hope to get anything insightful back out. We believe that we were able to process the data such that some meaningful information was extractable, but it is difficult to say what information was hidden by our transformations.


